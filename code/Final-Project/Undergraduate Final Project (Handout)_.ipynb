{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dISvnftHOqK0"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"m2cNp5f1Ouiq"},"source":["# Undergraduate Final Project \n","Worth 150 Points \\\\\n","Due Wednesday May 3rd at 5:00 PM \\\\\n","Student Name: "]},{"cell_type":"markdown","metadata":{"id":"3bKG8n3_PF87"},"source":["For your final project, you will be given two datasets (posted to Moodle) that have been used in reserach at ISU over the last 5 years. \\\\\n","\n","\n","1.   GPWR Dataset \n","2.   Battery Dataset \n","\n","\n","\n","In Section 1, you will use the Battery Dataset to demonstrate your knowledge of data exploration, data preperation, data visualization and regression. \\\\\n","\n","In Section 2 and 3, you will use the GPWR Dataset to demonstrate your knowledge of data exploration, data preperation, data visualization, supervised learning and unsupervised learning. \\\\\n","\n","Please note you will be writing a report on your findings, so make good use of comments and notes in your code. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"_MMCOXZGQhC_"},"source":["# Section 1 Regression (30 Points) "]},{"cell_type":"markdown","metadata":{"id":"FYWduuF4nF9y"},"source":["The battery dataset contains information relating to 121 lithium batteries. It contains 4 different features. The goal of your model is to predict the cycle of the battery using the four features. Our four features are parameter related to the capacity loss due to LLI of cell i, obtained from the initial cycle to cycle nQ. Essentially, they relate to battery capacity after some time. "]},{"cell_type":"markdown","source":["**1.0 Import Data**  \\\\\n","Import the battery dataset into Colab and print the header. "],"metadata":{"id":"FvkF2bcCRsUq"}},{"cell_type":"code","source":[],"metadata":{"id":"Sm_-RGMTgcXv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**1.1 Feature Engineering(5 Points)** \\\\\n","Researchers found there is a correlation between batteries at their End of Life (EOL) nQ=0.80 and the inverse of the cycle life at that capcity 1/n^0.8 or 1/cycle life, where n is the cycle life. Create a new column (1/ni^0.8) for each battery."],"metadata":{"id":"oIu99pgncut5"}},{"cell_type":"code","source":[],"metadata":{"id":"FfYEIs5vgev0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 1.2 Train a Random Forest Regression Model(5 Points)** \\\\\n","Reseracher have told us the data is not normally distributed. As such, linear regression likely won't produce a good model. We could fix this with a special technique called Box-Cox Transformation. However, for this assignment let's train a differnt model to predict our cycle life. Split the data (80/20), verfiy the dataset sizes, train a random forest model and store your predictions."],"metadata":{"id":"0lFvkKl_eKMM"}},{"cell_type":"code","source":[],"metadata":{"id":"JcrqkGbzglhG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 1.3 Model Evalution (10 Points)** \\\\\n","Now evaluate your model using RMSE and R^2. Is the model overfit/underfit? Extract the first tree from the model and plot it. Finally, print the feature importances for your model. "],"metadata":{"id":"1B8MqF3nhSYc"}},{"cell_type":"code","source":[],"metadata":{"id":"f-sqotQ1gqQU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 1.4 Explore Alternative Models (10 Points)**  \\\\\n","Now train three different regression models using other techniques (not linear regression) that we have discussed (kNN, SVR, etc.). Evaluate these using RMSE and R^2. Choose one of your models and attempt to improve it (Data scaling, hyper paramter tunning ,etc.) "],"metadata":{"id":"tYBgjtJkht9C"}},{"cell_type":"code","source":[],"metadata":{"id":"_cQ4kOusgq7t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9po4fMKmQoyz"},"source":["# Section 2 Supervised Learning (40 Points)"]},{"cell_type":"markdown","metadata":{"id":"fA8mBmrlQ04o"},"source":["The GPWR dataset contains over 110,000 datapoints from a reactor simulator at CAES. Over 160 simulations were run to collect this data.  The dataset contains data on 12 different reactor events, 11 Transient events (abnomral situations) and 1 Non-Transient (i.e. normal operations). \n","1.   Feedwater Pump Trip\n","2.   LOCA + LOOP & \n","3. Valve Closure \n","4. Rapid Power Change\n","5. Depressurization\n","6. Max Steam line Rupture\n","7. Manual Reactor Trip\n","8. Electrical Load Rejection\n","9. Single Coolant Pump Trip\n","10. Double CoolantPump Trip\n","11. Turbine Trip W/O SCRAM"]},{"cell_type":"markdown","metadata":{"id":"9YpZuiG1QrSX"},"source":["**2.0 Import the Data:** Setup the GPWR Dataset from Moodle in to Colab. To make your life easier, use the following list of feature names when you import the data. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHG28YacSneB"},"outputs":[],"source":["names=['TIME', 'NORMALIZED NEUTRON FLUX', 'RCS LVL LOOP 1 WR', 'RCS LVL LOOP 1 NR', 'HOT LEG 1 TEMPERATURE'\n","       ,  'COLD LEG 1A â€œTEMPERATURE', 'COLD LEG 1B TEMPERATURE', 'HOT LEG 2 TEMPERATURE', 'COLD LEG 2A TEMPERATURE', \n","       'COLD LEG 2B TEMPERATURE',  'RC LOOP-1A NORM FLOW', 'RC LOOP-1B NORM FLOW', 'RC LOOP-2A NORM FLOW', \n","       'RC LOOP-2B NORM FLOW',  'PZR SURGE LINE TEMP', 'PORV DISCH PZR TEMPERATURE',\n","       'CONTAINMENT PRESSURE', 'CONTAINMENT TEMPERATURE', 'SG-1 NR LEVEL', 'SG-2 NR LEVEL', \n","       'FW FLOW TO SG-1', 'FW FLOW TO SG-2', 'MS FLOW FROM SG-1 LINE-1A', 'MS FLOW FROM SG-1 LINE-1B', \n","       'MS FLOW FROM SG-2 LINE-2A', 'MS FLOW FROM SG-2 LINE-2B', 'SG-1 PRESSURE', 'SG-2 PRESSURE', \n","       'CALCULATED AVERAGE TEMPERATURE', 'PRESSURIZER PRESSURE', 'NORM PRESSURIZER LEVEL',\n","       'PRESSURIZER WATER TEMPERATURE', 'PRESSURIZER STEAM TEMPERATURE', 'GENERATOR POWER', \"REACTOR CORE LIFE\", \"TRANSIENT\"]"]},{"cell_type":"code","source":[],"metadata":{"id":"Am4MwWi7g1Ku"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L8Sodb3qUU_C"},"source":["**Section 2.1 Data Exploration (5 Points):** Before we start making changes to the dataset, let's explore the dataset. Using 3 differnt approaches, explore the dataset. (Keep in mind each must produce unique inferences for the data). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KogOJhIpV55f"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3Du87U28V8q3"},"source":["**Section 2.2: Data Preprocessing** (5 Points): Before we get into training this dataset, it needs some work.  "]},{"cell_type":"markdown","metadata":{"id":"VcIfjjUJWRkN"},"source":["If we look carefully, we will notice that a number of the features are percentages of actual values (which we have in the dataset). This makes these features  redundant. Let's get rid of them. Using your Python and pandas skills, drop the following features from your dataset: \n","\n","\n","1.   NORMALIZED NEUTRON FLUX\n","2.   RC LOOP-1A NORM FLOW\n","3.   RC LOOP-1B NORM FLOW\n","4.   RC LOOP-2A NORM FLOW\n","5.   RC LOOP-2B NORM FLOW\n","6.   SG-1 NR LEVEL\n","7.   FW FLOW TO SG-1\n","8.   NORM PRESSURIZER LEVEL\n","9. TIME \n","\n","(Note that in practice, we probably would hold on to the time feature for further analysis, but in this case, we will drop it). \n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"SRW3T3TMhI9X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l8_nsfRNXbGP"},"source":["Now let's look at the reactor core life feature. Determine how many unique values are in that feature and what these values are. "]},{"cell_type":"code","source":[],"metadata":{"id":"-ZJ4BR7KhM-5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ww4o6eQ-ZM9D"},"source":["It looks like the person in charge of entering in the data made a mistake. There should only be 3 values.  We need to fix this before we continue. Use your knowledge of Python, pandas and/or numpy to fix this. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYKNa-6Fae0G"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAiDgbUabLUf"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"rch9Zox6afuD"},"source":["You may have noticed there are some categorical features in this dataset. These data will not work with scikit learn. Convert all the categorical features into the *proper* numerical form. *Hint: double check your dataset afterward, you may have to drop the original categorical feature!*"]},{"cell_type":"code","source":[],"metadata":{"id":"wlfY2MOTP2Cw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZ1Ba-uVdAJ-"},"source":["Print the head of your data. You should have a dataset with 28 features. Double check that you have no Strings/Object values in your data. "]},{"cell_type":"code","source":[],"metadata":{"id":"eldoGd_thT57"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p6-2ua6_dv--"},"source":["**Section 2.3 Data Splitting & Feature Engineering (5 points)**  Split your data using Scikit Learn. Use a 50/50 split."]},{"cell_type":"markdown","metadata":{"id":"T2WQZZlGe7_w"},"source":["The TRANSIENT feature will be our target data for this. To make things simpler define y as the TRANSIENT Feature of your total dataset. Drop the TRANSIENT feature and define your remaining dataset as X. "]},{"cell_type":"code","source":[],"metadata":{"id":"bJxb6GuvhZuS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3PlmkNSVgnU4"},"source":["Split the data (use a 50/50 split) into a training and testing set. Verify the changes."]},{"cell_type":"code","source":[],"metadata":{"id":"W2qkbivXhY3n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZtmRUDcpg3Po"},"source":["Set aside any dummy variables in your dataset (we don't want to scale these).  Use one of the scaling/feature methods (Standard Scaler, Min-Max Scaler ,etc.) and fit to your training data (hint use .fit_transform()). Transform your testing data using the same scaler (use .transform()). Again, make sure not to scale the dummy variables. "]},{"cell_type":"code","source":[],"metadata":{"id":"nr1TrjQHhYVx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eUTkrfvSh6K3"},"source":["**Section 2.4 Model Training & Evaluation (10 Points):**\n","Select 3 supervised machine learning models (Decision tree, knn, etc.) and train a them using your training dataset to predict the reactor event occuring. Evaluate the model using accuracy, precision, recall, f1-score. Evaluate if your model is overfit. "]},{"cell_type":"code","source":[],"metadata":{"id":"0kLFwLCPhdJY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yw1kA_tAibTx"},"source":["**Section 2.5 Model Improvment (15 Points):**\n","Make 2 seperate attemps to improve each model. You can try a different scaler, adjust hyperparameters, use a different model type, etc. (You should have 6 models).Save the confusion matrix of the highest performing model for your report. \\\\\n","\n","**Note: You will not be graded on performance, just ensure that your model performs better than a random guess.**"]},{"cell_type":"code","source":[],"metadata":{"id":"adNHHfzahd5o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ahhohZCLjz9H"},"source":["# Section 3 Unsupervised Learning (50 Points)\n","You will use kmeans to develop and evaluate an unsupervised model using GPWR. "]},{"cell_type":"markdown","metadata":{"id":"Yv2PgH4x0NyP"},"source":["**Section 3.1 Deminsionality Reduction (10 Points)** \\\\\n","The first thing we want to do is visualize our data. However, our data has a high deminsionality (28 features). We need to reduce this dataset down to 2, so we can plot it on an X-Y plot. Use one of the deminsionality reduction techniques we discussed in class to reduce the GPWR dataset to 2 dimensions. "]},{"cell_type":"code","source":[],"metadata":{"id":"hN9pQ2rIhm1W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"suyM_QHW1yJS"},"source":["**Section 3.2 Training an Unsupervised Model(10 Points)** \\\\\n","Train a kmeans model using 12 clusters. Evaluate your model by plotting each of the 12 clusters. "]},{"cell_type":"code","source":[],"metadata":{"id":"sLmHm3IAhpiq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m62FJAvm3OfG"},"source":["**Section 3.3 Training more Kmeans Models (10 Points)** \\\\\n","Due to the nature of our data, we know there should be 12 groups. However, in most cases with unsupervised learning, we don't know this. Train a kmeans model for 2,4,6,8 & 10 clusters. Plot these clusters. "]},{"cell_type":"code","source":[],"metadata":{"id":"ZGfxPjQJhqvt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 3.4 Evaulating the Kmeans Models (10 Points)** \\\\\n","When working with unsupervised learning, we usually do not know any actual values, so we cannot calculate accuacy and other valiadation measurments (though in this case we could, since we have the actual results). Instead, we can use inertia and silhouette score to estimate our ideal number of clusters. Perform these calcualtions for your six kmeans models. Plot these values. In your report, evaluate how this number compares to the actual value we know (12). "],"metadata":{"id":"y1j5MJqJFcik"}},{"cell_type":"code","source":[],"metadata":{"id":"uL0kXzuSh06Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Ha_Kbv55jKX"},"source":["**Section 3.5 Trying Different Approaches with Unsupervised Learning (10 Points)** \\\\\n","Use DBSCAN Clustering to train a new unsupervised model using your GPWR data. Plot your clusters. In your report, compare how this performed to your 12 cluster kmeans model. "]},{"cell_type":"code","source":[],"metadata":{"id":"GQkepC4Ph2V6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LE87Y3-tjbTf"},"source":["## Final Report (30 Points)\n","Now that you have completed training all your models for both datasets, write a report between 2 and 3 pages, summarizing the work you did in each section. Include: \n","\n","\n","\n","1.   The data used \n","2.   The approach you used to prepare your data \n","3. The models you trained \n","4. The results from your models\n","\n","Include a seperate appendix, with all revelant figures from your work (you do not have to include the decision tree from Section 1 in the report). Be sure to include tables summarizing your models from each section (include the model type and all validation results). \n","Turn in a .doc or .pdf to moodle, along with your notebook. \n","\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1RQPpeI1SSL1hHk9FDpCrqqrTgnetyGtU","timestamp":1681488133142}],"authorship_tag":"ABX9TyN6COokMi1f+vebY87paKC7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}